# Copy to config.yaml and fill in secrets

supabase:
  url: "https://YOUR_PROJECT.supabase.co"
  service_role_key: "YOUR_SERVICE_ROLE_KEY"

scraping:
  httpx_delay_seconds: 1.5
  playwright_delay_seconds: 4
  max_pages_classifieds: 10
  max_pages_auctions: 50   # Full scrape (scheduler / CLI). Use 1 for quick test.
  # When run is triggered via POST /api/run (Odśwież oferty): cap pages and total listings. Daily/cron scrape is unaffected.
  # Omit to use max_pages_auctions for API runs too (may not finish on Railway).
  on_demand_max_pages_auctions: 10
  # Total listings to scrape on demand (default 20). First scraper gets up to this many; second gets remainder, etc.
  on_demand_max_listings: 20
  # Optional: only keep listings with auction_date in the last N days; stops pagination when older. Omit for full scrape.
  # days_back: 1
  # Active sources: komornik, e_licytacje, amw (Facebook via webhook). Omit or [] to run all.
  sources: ["komornik", "e_licytacje", "amw"]
  # Komornik: "" = all regions (each listing has region label for frontend filter). Set e.g. "świętokrzyskie" to limit.
  komornik_region: ""
  # E-licytacje: same as komornik — "" = all regions; set e.g. "świętokrzyskie" to limit. Region is parsed from detail (location parentheses).
  e_licytacje_region: ""

# Apify (Facebook): token do pobierania datasetów; webhook_secret do weryfikacji POST /webhook/apify
apify:
  token: "YOUR_APIFY_TOKEN"
  webhook_secret: ""

# On-demand run (POST /api/run, e.g. navbar refresh). If set, require header X-Run-Secret. If unset, apify.webhook_secret is used.
# run_api:
#   secret: "YOUR_RUN_SECRET"

logging:
  level: "INFO"
  rotation: "10 MB"
  retention: "7 days"

scheduler:
  enabled: true
  # Updates once per day at 13:07 CET (Europe/Warsaw)
  cron: "7 13 * * *"
  timezone: "Europe/Warsaw"
